#!/usr/bin/env python
# coding: utf-8

# importing python utility libraries
import os, sys, random, io, urllib
from datetime import datetime


os.environ['http_proxy'] = "http://squid-a10.prod.ice.int.threatmetrix.com:3128"
os.environ['https_proxy'] = "http://squid-a10.prod.ice.int.threatmetrix.com:3128"

from pathlib import Path
from utils.data_config import input_config
from utils.fraud_data import FraudData
from utils.tunning_related import prepare_pytorch_data,print_summary

# importing pytorch libraries
import torch
from torch import nn
from torch.utils.data import Dataset as BaseDataset
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F

# huggingface related
from datasets import Dataset
from transformers import MobileBertTokenizer
from transformers import BertForSequenceClassification
from torch.utils.data import TensorDataset, DataLoader, SequentialSampler
from transformers import BertConfig, BertForMaskedLM
from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, logging
from transformers import pipeline, TextClassificationPipeline
from transformers import BertTokenizer, BertModel
from transformers import AdamW
from transformers import get_scheduler

from sklearn.model_selection import train_test_split

# Tracking
from aim.hugging_face import AimCallback


# importing data science libraries
import pandas as pd
import numpy as np
import pickle as pkl


# importing python plotting libraries
import matplotlib.pyplot as plt
import seaborn as sns

# ### 1.2 CPU/GPU Device
torch.backends.cudnn.benchmark = True
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.cuda.empty_cache()

print(device)

### 1.4 Random Seed Initialization

# Finally, let' set the seeds of random elements in the code e.g. the initialization of the network parameters to guarantee deterministic computation and results:
# init deterministic seed
seed_value = 1234
random.seed(seed_value)  # set random seed
np.random.seed(seed_value)  # set numpy seed
torch.manual_seed(seed_value)  # set pytorch seed CPU
if (torch.backends.cudnn.version() != None):
    torch.cuda.manual_seed(seed_value)  # set pytorch seed GPU

## 2. Fraud Data
# Load feature data generated by Flagship model, Step 2
# borrow some codes from flagship model


### 2.1 Load the Parsed Data

#### load final features ######
with open('/home/zhanyi02/FeatureEngineering/checkpoints/featureimportance/plots/new/final_selected_features.npy',
          'rb') as f:
    vars_rc_list = np.load(f)

vars_rc_list = vars_rc_list.tolist()

vars_rc_list =  [x for x in vars_rc_list if  x.startswith(('tsrc','tmxrc'))]

print('total var length:', len(vars_rc_list))

csrt_train = pd.read_csv('/data/zhanyi02/trustscore/trustscore2023/combined/trust_score_2023_train.csv',usecols=vars_rc_list+['frd'])
print('train_dataset_size', csrt_train.shape)

train_df, val_df = train_test_split(csrt_train, test_size=0.2, random_state=42)

### 2.2 Check Data
X_train = train_df[vars_rc_list]
X_valid = val_df[vars_rc_list]
y_train = train_df['frd']
y_valid = val_df['frd']

vocab_file = "./checkpoints/nlp2023/v1/vocab.txt"
## do not rerun this code chunk

vocab = ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '[BOS]', '[EOS]']
for feature in vars_rc_list:
    vocab.append(feature + "_0")
    vocab.append(feature + "_1")

with open(vocab_file, 'w') as vocab_fp:
    vocab_fp.write("\n".join(vocab))

#### Tokenizer

special_tokens_dict = {"unk_token": "[UNK]",
                       "sep_token": "[SEP]",
                       "pad_token": "[PAD]",
                       "cls_token": "[CLS]",
                       "mask_token": "[MASK]",
                       "eos_token": "[EOS]",
                       "bos_token": "[BOS]"}

tokenizer = MobileBertTokenizer(vocab_file, do_basic_tokenize=False)
tokenizer.add_special_tokens(special_tokens_dict)



#### Transform Data
train_data_features = X_train.copy()
train_data_labels = y_train

##### fro validation
val_data_features = X_valid.copy()
val_data_labels = y_valid


# #### subset data for hyperparameter tuning
#
# train_data_subset = input_data.train.sample(frac=0.1)
# val_data_subset = input_data.val.sample(frac=0.1)
# test_data_subset = input_data.test.sample(frac=0.1)
#
# ###### for train
#
# train_data_features_subset = train_data_subset[input_data.predictors].copy()
# train_data_labels_subset = train_data_subset['frd']
#
# ##### for validation
#
# val_data_features_subset = val_data_subset[input_data.predictors].copy()
# val_data_labels_subset = val_data_subset['frd']
#
# ##### for test
# test_data_features_subset = test_data_subset[input_data.predictors].copy()
# test_data_labels_subset = test_data_subset['frd']

#### Preprocess Data

vocab_dict = tokenizer.get_vocab()

##### prepare train test
train_df = prepare_pytorch_data(train_data_features, train_data_labels, vocab_dict, device)
val_df = prepare_pytorch_data(val_data_features, val_data_labels, vocab_dict, device)

# train_df_subset = prepare_pytorch_data(train_data_features_subset, train_data_labels_subset, vocab_dict, device)
# val_df_subset = prepare_pytorch_data(val_data_features_subset, val_data_labels_subset, vocab_dict, device)
# test_df_subset = prepare_pytorch_data(test_data_features_subset, test_data_labels_subset, vocab_dict, device)


#### Train

config = BertConfig(vocab_size=len(vocab),hidden_size=256,num_hidden_layers=8,num_attention_heads=8)

model = BertForMaskedLM(config).to(device)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=True,mlm_probability=0.15)

optimizer = AdamW(model.parameters(), lr=6e-4,betas=(0.9,0.95),weight_decay=1e-1)


num_epochs = 6
num_training_steps = num_epochs * len(train_df)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=4000,
    num_training_steps=num_training_steps,
)
print(num_training_steps)

optimizers = optimizer, lr_scheduler

training_args = TrainingArguments(
    output_dir='./checkpoints/nlp2023/v2',
    per_device_train_batch_size=128,
    per_device_eval_batch_size=128,
    logging_steps=2000,
    save_strategy='steps',
    save_steps=2000,
    evaluation_strategy='steps',
    save_total_limit=2,
    load_best_model_at_end=True,
    num_train_epochs=num_epochs,
    report_to="wandb"
)
# aim_callback = AimCallback(repo='./checkpoints/nlp2023/v1', experiment='nlp2023_v1')


trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_df,
    eval_dataset=val_df,
    optimizers=optimizers,

    # callbacks=[aim_callback]

)

result = trainer.train()

print_summary(result)




# ##### logging
#
# # rest of the training args
# # ...
# training_args.logging_dir = 'logs' # or any dir you want to save logs
#
# # training
# train_result = trainer.train()
#
# # compute train results
# metrics = train_result.metrics
# max_train_samples = len(small_train_dataset)
# metrics["train_samples"] = min(max_train_samples, len(small_train_dataset))
#
# # save train results
# trainer.log_metrics("train", metrics)
# trainer.save_metrics("train", metrics)
#
# # compute evaluation results
# metrics = trainer.evaluate()
# max_val_samples = len(small_eval_dataset)
# metrics["eval_samples"] = min(max_val_samples, len(small_eval_dataset))
#
# # save evaluation results
# trainer.log_metrics("eval", metrics)
# trainer.save_metrics("eval", metrics)







#
#
# ### Train
# #### Train MLM with Hyperparameter Tuning- Optuna
# # - mixed the tuninng of model structure and training parameters together
# # - better to fix one and tune another one
#
# # aim_callback = AimCallback(experiment='huggingface_experiment')
#
# def objective(trial: optuna.Trial):
#     #     config = BertConfig(vocab_size=len(vocab),hidden_size=256,num_hidden_layers=4,num_attention_heads=4)
#
#     config = BertConfig(
#         vocab_size=len(vocab),
#         hidden_size=trial.suggest_categorical("hidden_size", [256, 512]),
#         num_hidden_layers=trial.suggest_categorical("num_hidden_layers", [4, 6, 8]),
#         num_attention_heads=trial.suggest_categorical("num_attention_heads", [4, 8]))
#
#     model = BertForMaskedLM(config).to(device)
#
#     data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)
#
#     #     training_args = TrainingArguments(
#     #         output_dir='./checkpoints',
#     #         per_device_train_batch_size=trial.suggest_categorical("per_device_train_batch_size", [16, 32, 64, 128]),
#     #         learning_rate=trial.suggest_loguniform("learning_rate", low=4e-5, high=0.01),
#     #         weight_decay=trial.suggest_loguniform("weight_decay", 4e-5, 0.01),
#     # #         num_train_epochs=trial.suggest_int("num_train_epochs", low=2, high=5),
#     #         num_train_epochs=4,save_strategy='steps',evaluation_strategy='steps',max_steps=4000,
#     #         logging_steps=100, save_steps=200, eval_steps=100, save_total_limit=2,load_best_model_at_end=True,
#     #     )
#
#     training_args = TrainingArguments(
#         output_dir='./checkpoints/tuningtest',
#         per_device_train_batch_size=128,
#         per_device_eval_batch_size=128,
#         num_train_epochs=1, save_strategy='epoch', evaluation_strategy='epoch',
#         logging_steps=100, save_total_limit=2, load_best_model_at_end=True,
#     )
#
#     trainer = Trainer(
#         model=model,
#         args=training_args,
#         data_collator=data_collator,
#         train_dataset=train_df_subset,
#         eval_dataset=val_df_subset,
#     )
#     result = trainer.train()
#     return result.training_loss
#
#
# # 3. Create a study object and optimize the objective function.
# # tensorboard_callback = TensorBoardCallback("./checkpoints/logs", metric_name="loss")
#
# study = optuna.create_study(direction='minimize', study_name="MLM")
# study.optimize(objective, n_trials=2)
#
# # Create a dataframe from the study.
# df_studies = study.trials_dataframe()
#
# df_studies.to_csv("./checkpoints/tuningtest/plots/studies.csv")
#
# print("Best params: ", study.best_params)
# print("Best value: ", study.best_value)
# print("Best Trial: ", study.best_trial)
# print("Trials: ", study.trials)
#
# fig_history = plot_optimization_history(study)
# fig_importance = plot_param_importances(study)
# fig_parallel = plot_parallel_coordinate(study)
# fig_contour = plot_contour(study)
#
# fig_history.write_image("./checkpoints/tuningtest/plots/plot_optimization_history.pdf")
# fig_importance.write_image("./checkpoints/tuningtest/plots/plot_param_importances.pdf")
# fig_parallel.write_image("./checkpoints/tuningtest/plots/plot_parallel_coordinate.pdf")
# fig_contour.write_image("./checkpoints/tuningtest/plots/plot_contour.pdf")

